{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppressing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\admin\\anaconda3\\lib\\site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-22.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for pathtools: [Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\pathtools-0.1.2.dist-info\\\\METADATA'\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.9.2-cp37-cp37m-win_amd64.whl (444.1 MB)\n",
      "Collecting numpy>=1.20\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-win_amd64.whl (1.5 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.7.0-cp37-cp37m-win_amd64.whl (2.6 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n",
      "Processing c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Using cached tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-14.0.6-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Using cached tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.4-cp37-cp37m-win_amd64.whl (896 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.48.1-cp37-cp37m-win_amd64.whl (3.6 MB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting pyparsing!=3.0.5,>=2.0.2\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.11.0-py2.py3-none-any.whl (167 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
      "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.1-cp37-cp37m-win_amd64.whl (17 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: numpy, tensorflow-io-gcs-filesystem, wrapt, pyparsing, packaging, six, google-pasta, h5py, gast, setuptools, termcolor, wheel, astunparse, opt-einsum, keras, absl-py, charset-normalizer, idna, urllib3, certifi, requests, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard-data-server, typing-extensions, zipp, importlib-metadata, markdown, tensorboard-plugin-wit, MarkupSafe, werkzeug, protobuf, grpcio, tensorboard, libclang, keras-preprocessing, flatbuffers, tensorflow-estimator, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\n",
      "ERROR: pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\n",
      "ERROR: conda 4.12.0 requires ruamel_yaml_conda>=0.11.14, which is not installed.\n",
      "ERROR: astroid 2.3.3 requires typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\", which is not installed.\n",
      "ERROR: astroid 2.3.3 has requirement wrapt==1.11.*, but you'll have wrapt 1.14.1 which is incompatible.\n",
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Users\\\\admin\\\\anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\.libs\\\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install --ignore-installed --upgrade tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\admin\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - numpy=1.15\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    astropy-3.2.3              |   py37he774522_0         5.6 MB\n",
      "    numpy-1.15.4               |   py37h19fb1c0_0          46 KB\n",
      "    numpy-base-1.15.4          |   py37hc3f5095_0         3.1 MB\n",
      "    scikit-learn-0.23.2        |   py37h47e9c7a_0         4.6 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        13.3 MB\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  astropy                                4.0-py37he774522_0 --> 3.2.3-py37he774522_0\n",
      "  numpy                               1.18.1-py37h93ca92e_0 --> 1.15.4-py37h19fb1c0_0\n",
      "  numpy-base                          1.18.1-py37hc3f5095_1 --> 1.15.4-py37hc3f5095_0\n",
      "  scikit-learn                         1.0.2-py37hf11a4ad_1 --> 0.23.2-py37h47e9c7a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "numpy-base-1.15.4    | 3.1 MB    |            |   0% \n",
      "numpy-base-1.15.4    | 3.1 MB    |            |   1% \n",
      "numpy-base-1.15.4    | 3.1 MB    | #4         |  14% \n",
      "numpy-base-1.15.4    | 3.1 MB    | ###        |  31% \n",
      "numpy-base-1.15.4    | 3.1 MB    | ####5      |  46% \n",
      "numpy-base-1.15.4    | 3.1 MB    | #####9     |  60% \n",
      "numpy-base-1.15.4    | 3.1 MB    | #######4   |  75% \n",
      "numpy-base-1.15.4    | 3.1 MB    | #########  |  90% \n",
      "numpy-base-1.15.4    | 3.1 MB    | ########## | 100% \n",
      "\n",
      "astropy-3.2.3        | 5.6 MB    |            |   0% \n",
      "astropy-3.2.3        | 5.6 MB    |            |   0% \n",
      "astropy-3.2.3        | 5.6 MB    | 3          |   3% \n",
      "astropy-3.2.3        | 5.6 MB    | 6          |   6% \n",
      "astropy-3.2.3        | 5.6 MB    | 9          |   9% \n",
      "astropy-3.2.3        | 5.6 MB    | #4         |  15% \n",
      "astropy-3.2.3        | 5.6 MB    | ##1        |  22% \n",
      "astropy-3.2.3        | 5.6 MB    | ##9        |  29% \n",
      "astropy-3.2.3        | 5.6 MB    | ###8       |  38% \n",
      "astropy-3.2.3        | 5.6 MB    | ####7      |  47% \n",
      "astropy-3.2.3        | 5.6 MB    | #####5     |  55% \n",
      "astropy-3.2.3        | 5.6 MB    | ######4    |  64% \n",
      "astropy-3.2.3        | 5.6 MB    | #######2   |  72% \n",
      "astropy-3.2.3        | 5.6 MB    | ########1  |  81% \n",
      "astropy-3.2.3        | 5.6 MB    | ########9  |  90% \n",
      "astropy-3.2.3        | 5.6 MB    | #########8 |  98% \n",
      "astropy-3.2.3        | 5.6 MB    | ########## | 100% \n",
      "\n",
      "numpy-1.15.4         | 46 KB     |            |   0% \n",
      "numpy-1.15.4         | 46 KB     | ########## | 100% \n",
      "numpy-1.15.4         | 46 KB     | ########## | 100% \n",
      "\n",
      "scikit-learn-0.23.2  | 4.6 MB    |            |   0% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | 6          |   7% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | #8         |  18% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | ##7        |  28% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | ###8       |  39% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | ####9      |  49% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | #####9     |  60% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | #######    |  70% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | ########   |  80% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | #########  |  90% \n",
      "scikit-learn-0.23.2  | 4.6 MB    | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.14.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install numpy=1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "#import tensorflow\n",
    "from numpy import loadtxt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you will use the Pima Indians onset of diabetes dataset. This is a standard machine learning dataset from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
    "\n",
    "- As such, it is a binary classification problem (onset of diabetes as 1 or not as 0). All of the input variables that describe each patient are numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our first neural network in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables can be summarized as follows:\n",
    "\n",
    "Input Variables (X):\n",
    "- Number of times pregnant\n",
    "- Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "- Diastolic blood pressure (mm Hg)\n",
    "- Triceps skin fold thickness (mm)\n",
    "- 2-hour serum insulin (mu U/ml)\n",
    "- Body mass index (weight in kg/(height in m)^2)\n",
    "- Diabetes pedigree function\n",
    "- Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "- Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "The dataset has nine columns, and the range 0:8 will select columns from 0 to 7, stopping before index 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "model=Sequential()\n",
    "model.add(Dense(12,input_shape=(8,),activation='relu'))\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Keras Model\n",
    "- Now that the model is defined, you can compile it.\n",
    "\n",
    "- Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU, GPU, or even distributed.\n",
    "\n",
    "- When compiling, you must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in your dataset.\n",
    "\n",
    "- You must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics you want to collect and report during training.\n",
    "\n",
    "- In this case, use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“. You can learn more about choosing loss functions based on your problem here:\n",
    "\n",
    "- How to Choose Loss Functions When Training Deep Learning Neural Networks\n",
    "- We will define the optimizer as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\admin\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  for to display neural network we need to install these libraries\n",
    "- pip3 install ann_visualizer\n",
    "- pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 44.0 kB/s eta 0:00:00\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for pathtools: [Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\pathtools-0.1.2.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ann_visualizer\n",
      "  Using cached ann_visualizer-2.5.tar.gz (4.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [14 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 36, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 14, in <module>\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 18, in <module>\n",
      "      from setuptools.dist import Distribution\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\setuptools\\dist.py\", line 34, in <module>\n",
      "      from ._importlib import metadata\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\setuptools\\_importlib.py\", line 39, in <module>\n",
      "      disable_importlib_metadata_finder(metadata)\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\setuptools\\_importlib.py\", line 30, in disable_importlib_metadata_finder\n",
      "      for ob in sys.meta_path\n",
      "    File \"C:\\Users\\admin\\anaconda3\\lib\\site-packages\\setuptools\\_importlib.py\", line 31, in <listcomp>\n",
      "      if isinstance(ob, importlib_metadata.MetadataPathFinder)\n",
      "  AttributeError: module 'importlib_metadata' has no attribute 'MetadataPathFinder'\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install ann_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Using cached graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing requirements for pathtools: [Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\anaconda3\\\\lib\\\\site-packages\\\\pathtools-0.1.2.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\admin\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ann_visualizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-19b37fef7655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mann_visualizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mann_viz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mann_viz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"network.gv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ann_visualizer'"
     ]
    }
   ],
   "source": [
    "from ann_visualizer.visualize import ann_viz  \n",
    "from graphviz import Source\n",
    "ann_viz(model, view=True, title=\"test\", filename=\"network.gv\")\n",
    "\n",
    "\n",
    "\n",
    "#from ann_visualizer.visualize import ann_viz;\n",
    "\n",
    "#ann_viz(model, title=\"My first neural network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Keras Model\n",
    "You have defined your model and compiled it to get ready for efficient computation.\n",
    "\n",
    "Now it is time to execute the model on some data.\n",
    "\n",
    "You can train or fit your model on your loaded data by calling the fit() function on the model.\n",
    "\n",
    "Training occurs over epochs, and each epoch is split into batches.\n",
    "\n",
    "- Epoch: One pass through all of the rows in the training dataset\n",
    "- Batch: One or more samples considered by the model within an epoch before weights are updated\n",
    "\n",
    "One epoch comprises one or more batches, based on the chosen batch size, and the model is fit for many epochs. For more on the difference between epochs and batches, see the post:\n",
    "\n",
    "What is the Difference Between a Batch and an Epoch in a Neural Network?\n",
    "The training process will run for a fixed number of epochs (iterations) through the dataset that you must specify using the epochs argument. You must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size, and set using the batch_size argument.\n",
    "\n",
    "This problem will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
    "\n",
    "These configurations can be chosen experimentally by trial and error. You want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 1s 2ms/sample - loss: 13.7289 - acc: 0.6107\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 240us/sample - loss: 1.5809 - acc: 0.5339\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 182us/sample - loss: 0.8683 - acc: 0.5755\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 182us/sample - loss: 0.7368 - acc: 0.6068\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 181us/sample - loss: 0.7150 - acc: 0.6120\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 209us/sample - loss: 0.6826 - acc: 0.6445\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.6600 - acc: 0.6497\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.6352 - acc: 0.6784\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 263us/sample - loss: 0.6319 - acc: 0.6667\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.6487 - acc: 0.6576\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.6356 - acc: 0.6745\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 219us/sample - loss: 0.6238 - acc: 0.6810\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.6019 - acc: 0.6940\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5992 - acc: 0.6927\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 240us/sample - loss: 0.5894 - acc: 0.7031\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 223us/sample - loss: 0.5989 - acc: 0.6953\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 240us/sample - loss: 0.5965 - acc: 0.6823\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 271us/sample - loss: 0.5938 - acc: 0.6888\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5963 - acc: 0.7122\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5905 - acc: 0.7031\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 190us/sample - loss: 0.6042 - acc: 0.6641\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 174us/sample - loss: 0.5943 - acc: 0.6940\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5809 - acc: 0.7096\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 235us/sample - loss: 0.6019 - acc: 0.6966\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 219us/sample - loss: 0.5788 - acc: 0.7031\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5911 - acc: 0.7109\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 240us/sample - loss: 0.5898 - acc: 0.7044\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 200us/sample - loss: 0.6022 - acc: 0.6992\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.6121 - acc: 0.6810\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5833 - acc: 0.7070\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5820 - acc: 0.7044\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5804 - acc: 0.6940\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5777 - acc: 0.7005\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5780 - acc: 0.6966\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5612 - acc: 0.7135\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5896 - acc: 0.7057\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5833 - acc: 0.6875\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5621 - acc: 0.7174\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5710 - acc: 0.6901\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5827 - acc: 0.7044\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 210us/sample - loss: 0.5661 - acc: 0.7161\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5704 - acc: 0.6992\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5609 - acc: 0.7318\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5699 - acc: 0.7057\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5679 - acc: 0.7031\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5952 - acc: 0.6927\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5770 - acc: 0.7018\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5544 - acc: 0.7227\n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 178us/sample - loss: 0.5645 - acc: 0.7148\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5696 - acc: 0.7109\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 193us/sample - loss: 0.5710 - acc: 0.7096\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5658 - acc: 0.7018\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 267us/sample - loss: 0.5565 - acc: 0.7174\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 200us/sample - loss: 0.5754 - acc: 0.7174\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5628 - acc: 0.7096\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5579 - acc: 0.7122\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5674 - acc: 0.7031\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5510 - acc: 0.7214\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 186us/sample - loss: 0.5531 - acc: 0.7201\n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 210us/sample - loss: 0.5553 - acc: 0.7318\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5554 - acc: 0.7227\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 229us/sample - loss: 0.5708 - acc: 0.7096\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5575 - acc: 0.7070\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 226us/sample - loss: 0.5668 - acc: 0.7109\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 205us/sample - loss: 0.5544 - acc: 0.7214\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5548 - acc: 0.7266\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 229us/sample - loss: 0.5467 - acc: 0.7318\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 178us/sample - loss: 0.5442 - acc: 0.7201\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5534 - acc: 0.7279\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 219us/sample - loss: 0.5500 - acc: 0.7253\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5585 - acc: 0.7174\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5486 - acc: 0.7279\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 221us/sample - loss: 0.5449 - acc: 0.7201\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5406 - acc: 0.7448\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5452 - acc: 0.7383\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 219us/sample - loss: 0.5470 - acc: 0.7331\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5484 - acc: 0.7122\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5471 - acc: 0.7253\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5480 - acc: 0.7435\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5467 - acc: 0.7292\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5384 - acc: 0.7331\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5494 - acc: 0.7122\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5606 - acc: 0.7240\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5463 - acc: 0.7227\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 190us/sample - loss: 0.5388 - acc: 0.7487\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5453 - acc: 0.7370\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5354 - acc: 0.7253\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5393 - acc: 0.7318\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5356 - acc: 0.7396\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5371 - acc: 0.7266\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 178us/sample - loss: 0.5312 - acc: 0.7435\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5301 - acc: 0.7500\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 178us/sample - loss: 0.5554 - acc: 0.7409\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5620 - acc: 0.7370\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5410 - acc: 0.7383\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5296 - acc: 0.7227\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 167us/sample - loss: 0.5367 - acc: 0.7422\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 173us/sample - loss: 0.5528 - acc: 0.7161\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5339 - acc: 0.7266\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5344 - acc: 0.7370\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5356 - acc: 0.7305\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5428 - acc: 0.7383\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5324 - acc: 0.7383\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5397 - acc: 0.7266\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5274 - acc: 0.7474\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5186 - acc: 0.7500\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5241 - acc: 0.7487\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5238 - acc: 0.7474\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5274 - acc: 0.7539\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5245 - acc: 0.7513\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5578 - acc: 0.7292\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5355 - acc: 0.7474\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 167us/sample - loss: 0.5341 - acc: 0.7396\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 175us/sample - loss: 0.5224 - acc: 0.7383\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 250us/sample - loss: 0.5232 - acc: 0.7539\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5194 - acc: 0.7539\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5293 - acc: 0.7357\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 196us/sample - loss: 0.5235 - acc: 0.7422\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 180us/sample - loss: 0.5389 - acc: 0.7409\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5134 - acc: 0.7513\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5279 - acc: 0.7422\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5171 - acc: 0.7474\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5259 - acc: 0.7448\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5192 - acc: 0.7461\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 178us/sample - loss: 0.5176 - acc: 0.7591\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.5224 - acc: 0.7331\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 183us/sample - loss: 0.5106 - acc: 0.7539\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5083 - acc: 0.7591\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 201us/sample - loss: 0.5149 - acc: 0.7513\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.5240 - acc: 0.7513\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5134 - acc: 0.7630\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5142 - acc: 0.7448\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 181us/sample - loss: 0.4971 - acc: 0.7773\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5222 - acc: 0.7409\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5108 - acc: 0.7513\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5046 - acc: 0.7448\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.4913 - acc: 0.7708\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 177us/sample - loss: 0.5284 - acc: 0.7422\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 189us/sample - loss: 0.5157 - acc: 0.7552\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 167us/sample - loss: 0.5085 - acc: 0.7487\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.5098 - acc: 0.7630\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5145 - acc: 0.7591\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 198us/sample - loss: 0.5069 - acc: 0.7487\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.5108 - acc: 0.7643\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 197us/sample - loss: 0.5025 - acc: 0.7604\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 179us/sample - loss: 0.4975 - acc: 0.7578\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 240us/sample - loss: 0.5030 - acc: 0.7708\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 188us/sample - loss: 0.4953 - acc: 0.7682\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 208us/sample - loss: 0.4946 - acc: 0.7617\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 187us/sample - loss: 0.4992 - acc: 0.7721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x215ef06fd08>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Keras Model\n",
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset.\n",
    "\n",
    "This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy), but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset using the evaluate() function and pass it the same input and output used to train the model.\n",
    "\n",
    "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
    "\n",
    "The evaluate() function will return a list with two values. The first will be the loss of the model on the dataset, and the second will be the accuracy of the model on the dataset. You are only interested in reporting the accuracy so ignore the loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.64583134651184\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "accuracy=model.evaluate(X,y,verbose=0)\n",
    "#print('Accuracy: %.2f' % (accuracy*100))\n",
    "print('Accuracy:',accuracy[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = model.predict(X)\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
